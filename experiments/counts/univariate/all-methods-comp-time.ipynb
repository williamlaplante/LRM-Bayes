{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a236f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from LRM.CMP1D import CMP1D, SmoothedEmpiricalDensity1D\n",
    "\n",
    "from DiscreteFisherBayes.Source.Models import CMP\n",
    "from DiscreteFisherBayes.Source.Posteriors import FDBayes, Bayes\n",
    "import time\n",
    "\n",
    "theta1 = 4.0\n",
    "theta2 = 1.25\n",
    "dnum = 3000\n",
    "pnum = 5000\n",
    "numboot = 100\n",
    "\n",
    "RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443af711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data...\n"
     ]
    }
   ],
   "source": [
    "#This part simulates data that we will use for each method to evaluate their cost.\n",
    "\n",
    "if RUN:\n",
    "    prior = torch.distributions.Chi2(torch.tensor([3.0, 3.0]))\n",
    "    log_prior = lambda param: prior.log_prob(param).sum()\n",
    "    transit_p = torch.distributions.Normal(torch.zeros(2), 0.1*torch.ones(2))\n",
    "    \n",
    "    #Run if needed only (it's 100k samples so takes a bit of time)\n",
    "    #NM = 10000\n",
    "    Ns = [250, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "    NM = 100\n",
    "    p0 = prior.sample()\n",
    "    cmp = CMP()\n",
    "    full_data = cmp.sample(torch.tensor([theta1, theta2]), 10*NM*100, 20000)[::100,:]\n",
    "\n",
    "    datas = []\n",
    "    print(\"Data...\")\n",
    "    for ith in range(len(Ns)):\n",
    "        datas.append([])\n",
    "        for jth in range(10):\n",
    "            idxj = torch.randint(10*NM, (Ns[ith],))\n",
    "            datas[ith].append(full_data[idxj,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b31f8",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3875e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation...\n",
      "0\n",
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5939.76it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5668.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5981.63it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6128.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5916.63it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6084.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6211.13it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5933.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6241.12it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6166.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6037.14it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6137.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6245.73it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5945.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6078.16it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5967.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5884.21it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5884.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5964.06it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6019.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5978.72it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5728.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5902.29it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5838.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5833.30it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5929.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6016.85it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5933.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5834.64it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5902.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5875.93it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5771.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5872.41it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5857.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5159.03it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5619.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5860.61it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5797.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5612.97it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5818.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5262.00it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5050.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5633.19it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5659.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5532.65it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5694.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5764.63it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5548.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5768.26it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5652.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5550.72it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5715.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5721.95it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5483.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5748.08it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5670.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5610.63it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5697.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5766.61it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5513.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5542.49it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5488.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5366.59it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5491.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5555.10it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5298.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5544.51it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5458.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5380.07it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5506.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5518.24it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5124.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4569.84it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5090.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4793.70it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5446.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5339.02it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5277.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5577.36it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5487.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5154.93it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5233.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5340.72it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5052.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5340.86it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5294.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5148.09it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5290.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5265.50it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5130.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5349.97it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5281.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5157.95it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5189.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5303.34it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4199.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5225.71it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4909.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5246.90it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5105.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4788.69it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4938.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4955.85it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5084.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5016.82it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4471.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5119.12it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5080.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3116.07it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5066.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5125.95it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3745.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2805.42it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4523.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5099.08it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5050.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4900.17it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5053.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4939.81it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5071.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4952.38it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4733.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4918.69it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4884.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4805.10it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4907.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4822.56it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4776.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3711.22it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4024.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4610.91it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4463.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4541.27it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4829.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4657.94it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4881.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4904.71it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4777.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4968.43it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4908.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4680.57it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3383.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3468.16it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4607.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4759.22it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4619.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4728.00it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4567.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4576.96it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4737.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4624.76it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4585.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4444.91it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4549.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4686.35it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4407.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4513.38it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4647.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4603.43it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4693.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_MCMC_samples = 5000\n",
    "Ps = np.zeros((len(Ns), 10, num_MCMC_samples, 2))\n",
    "Ts_P = np.zeros((len(Ns), 10))\n",
    "Ts_P_total = np.zeros((len(Ns), 10))\n",
    "\n",
    "\n",
    "if RUN:\n",
    "    for args in [\"Bayes\"]:\n",
    "\n",
    "\n",
    "        print(\"Computation...\")\n",
    "        for ith in range(len(Ns)):\n",
    "            print(ith)\n",
    "\n",
    "            for jth in range(10):\n",
    "                data = datas[ith][jth]\n",
    "                print(\"Dataset size: \", data.shape)\n",
    "                \n",
    "                time_start = time.time()\n",
    "                \n",
    "                beta_opt = torch.tensor([1.0])\n",
    "                    \n",
    "                time_mid = time.time()\n",
    "                posterior = Bayes(cmp.uloglikelihood, torch.arange(500).reshape(500, 1), log_prior)\n",
    "                posterior.set_X(data)\n",
    "                post_sample_beta = posterior.sample(num_MCMC_samples, num_MCMC_samples, transit_p, prior.sample(), beta=beta_opt)\n",
    "                time_end = time.time()\n",
    "\n",
    "                Ts_P_total[ith, jth] = time_end - time_start\n",
    "                Ts_P[ith, jth] = time_end - time_mid #just inference/MCMC times\n",
    "\n",
    "                Ps[ith, jth, :, :] = post_sample_beta.numpy()\n",
    "        \n",
    "        print(\"Saving...\")\n",
    "        np.savez(f\"./outputs/computation_{args}_theta1={theta1}_theta2={theta2}_numMCMCsamples={num_MCMC_samples}.npz\", n=Ns, times_total=Ts_P_total, times=Ts_P, data=full_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee73f5",
   "metadata": {},
   "source": [
    "# DFD-Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c19a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation...\n",
      "0\n",
      "Dataset size:  torch.Size([250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -390.40457: 100%|██████████| 100/100 [01:36<00:00,  1.04it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6706.34it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7225.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Dataset size:  torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -723.89771: 100%|██████████| 100/100 [01:37<00:00,  1.03it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6774.80it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6032.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Dataset size:  torch.Size([750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -1051.60095: 100%|██████████| 100/100 [01:40<00:00,  1.01s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6519.16it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6094.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Dataset size:  torch.Size([1000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -1447.52856: 100%|██████████| 100/100 [01:42<00:00,  1.03s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6209.52it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5814.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Dataset size:  torch.Size([1250, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -1826.60999: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5887.65it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5824.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Dataset size:  torch.Size([1500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -2108.85229: 100%|██████████| 100/100 [01:55<00:00,  1.15s/it]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5252.01it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5409.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Dataset size:  torch.Size([1750, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -2457.17017: 100%|██████████| 100/100 [01:57<00:00,  1.18s/it]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4983.70it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4780.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Dataset size:  torch.Size([2000, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Loss: -2832.85693: 100%|██████████| 100/100 [02:00<00:00,  1.20s/it]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4737.62it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4939.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_MCMC_samples = 5000\n",
    "Ps = np.zeros((len(Ns), 1, num_MCMC_samples, 2))\n",
    "Ts_P = np.zeros((len(Ns), 1))\n",
    "Ts_P_total = np.zeros((len(Ns), 1))\n",
    "\n",
    "def get_beta_opt(posterior, data):\n",
    "        \n",
    "    Ps = np.zeros((10, pnum, 2))\n",
    "    beta_opt = torch.tensor([1.0])\n",
    "\n",
    "    p_init, _ = posterior.minimise(posterior.loss, prior.sample(), ite=50000, lr=0.1, loss_thin=100, progress=False)\n",
    "    boot_minimisers, _ = posterior.bootstrap_minimisers(data, numboot, lambda: p_init)\n",
    "    posterior.set_X(data)\n",
    "    beta_opt = posterior.optimal_beta(posterior.loss, boot_minimisers)\n",
    "    return beta_opt\n",
    "\n",
    "\n",
    "if RUN:\n",
    "    for args in [\"FDBayes\"]:\n",
    "    \n",
    "        print(\"Computation...\")\n",
    "        for ith in range(len(Ns)):\n",
    "            print(ith)\n",
    "\n",
    "            for jth in range(1):\n",
    "                data = datas[ith][jth]\n",
    "                print(\"Dataset size: \", data.shape)\n",
    "                                \n",
    "                time_start = time.time()\n",
    "                posterior = FDBayes(cmp.ratio_m, cmp.ratio_p, cmp.stat_m, cmp.stat_p, log_prior)\n",
    "                posterior.set_X(data)\n",
    "\n",
    "                beta_opt = get_beta_opt(posterior, data)\n",
    "                    \n",
    "                time_mid = time.time()\n",
    "                post_sample_beta = posterior.sample(num_MCMC_samples, num_MCMC_samples, transit_p, prior.sample(), beta=beta_opt)\n",
    "                time_end = time.time()\n",
    "\n",
    "                Ts_P_total[ith, jth] = time_end - time_start\n",
    "                Ts_P[ith, jth] = time_end - time_mid #just inference/MCMC times\n",
    "\n",
    "                Ps[ith, jth, :, :] = post_sample_beta.numpy()\n",
    "        \n",
    "        print(\"Saving...\")\n",
    "        np.savez(f\"./outputs/computation_{args}_theta1={theta1}_theta2={theta2}_numMCMCsamples={num_MCMC_samples}.npz\", n=Ns, times_total=Ts_P_total, times=Ts_P, data=full_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eaa338",
   "metadata": {},
   "source": [
    "# LRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4573202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_theta_logtheta(mu_X, Sigma_X):\n",
    "\n",
    "    mu_X = np.asarray(mu_X)\n",
    "    Sigma_X = np.asarray(Sigma_X)\n",
    "\n",
    "    theta1, theta2 = mu_X\n",
    "    if theta1 <= 0:\n",
    "        raise ValueError(\"theta1 must be positive for log transformation.\")\n",
    "\n",
    "    # g(mu_X)\n",
    "    mu_Y = np.array([np.log(theta1), theta2])\n",
    "\n",
    "    # Jacobian of g at mu_X\n",
    "    J = np.array([\n",
    "        [1/theta1, 0],\n",
    "        [0,        1]\n",
    "    ])\n",
    "\n",
    "    # Covariance after transformation\n",
    "    Sigma_Y = J @ Sigma_X @ J.T\n",
    "\n",
    "    return mu_Y.reshape(-1,1), Sigma_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e4865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.229144, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.557997, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.557759, coverage: 0.8200 (target 0.9500); fun=0.0169, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.215994, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.345047, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.413476, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.402504, coverage: 0.9800 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.657923, coverage: 0.2000 (target 0.9500); fun=0.5625, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.461253, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (250,)\n",
      "[scipy] beta*: 0.55801, coverage: 0.8800 (target 0.9500); fun=0.0049, success=True\n",
      "1\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.360138, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.323487, coverage: 0.9200 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.381098, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.467569, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.476602, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.344859, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.573823, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.317755, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.471447, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (500,)\n",
      "[scipy] beta*: 0.307977, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "2\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.436453, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.345047, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.557971, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.291108, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.268048, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.447445, coverage: 0.9800 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.345056, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.384387, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.6853, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (750,)\n",
      "[scipy] beta*: 0.286766, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "3\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.447445, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.842774, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.658517, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.447445, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.462038, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.557863, coverage: 0.9800 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.479962, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.437825, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.26265, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1000,)\n",
      "[scipy] beta*: 0.445482, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "4\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.664967, coverage: 0.9200 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.730778, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.621438, coverage: 0.9800 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.508405, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.362438, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.306976, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 1.45958, coverage: 0.8600 (target 0.9500); fun=0.0081, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.522182, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.558989, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1250,)\n",
      "[scipy] beta*: 0.55776, coverage: 0.9200 (target 0.9500); fun=0.0009, success=True\n",
      "5\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.475102, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.557936, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.902287, coverage: 0.9000 (target 0.9500); fun=0.0025, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 1.45945, coverage: 0.7400 (target 0.9500); fun=0.0441, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.421202, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.586027, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.376401, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.487131, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.345104, coverage: 0.9800 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (1500,)\n",
      "[scipy] beta*: 0.902309, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "6\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.902211, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.537464, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.521175, coverage: 0.9400 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.424172, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.345047, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.829098, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.550325, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.553588, coverage: 0.9200 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.481827, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (1750,)\n",
      "[scipy] beta*: 0.42457, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "7\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.640887, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.405388, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.795244, coverage: 0.9200 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.558003, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.465073, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.479779, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.730778, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.902363, coverage: 0.9200 (target 0.9500); fun=0.0009, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.322573, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Dataset size:  (2000,)\n",
      "[scipy] beta*: 0.449752, coverage: 0.9600 (target 0.9500); fun=0.0001, success=True\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "prior = multivariate_normal(mean=[3.0, 3.0], cov=[[1.0, 0.0], [0.0, 1.0]])\n",
    "prior_mean, prior_cov = transform_theta_logtheta(prior.mean, prior.cov)\n",
    "\n",
    "Ps = np.zeros((len(Ns), 10, 5000, 2))\n",
    "Ts_P = np.zeros((len(Ns), 10))\n",
    "Ts_P_total = np.zeros((len(Ns), 10))\n",
    "\n",
    "if RUN:\n",
    "    for ith in range(len(Ns)):\n",
    "        print(ith)\n",
    "\n",
    "        for jth in range(10):\n",
    "            data = datas[ith][jth]\n",
    "            data = data.numpy().flatten()\n",
    "            print(\"Dataset size: \", data.shape)\n",
    "            \n",
    "            \n",
    "            time_start = time.time()\n",
    "            empirical = SmoothedEmpiricalDensity1D(alpha=0.0)\n",
    "            empirical.fit(data)\n",
    "            cmp_lrm = CMP1D(empirical=empirical)\n",
    "            beta, _, _ = cmp_lrm.fit_coverage(data=data, prior_mean=prior_mean, prior_cov=prior_cov, verbose=True, B=50)\n",
    "\n",
    "            time_mid = time.time()\n",
    "            posterior = cmp_lrm.posterior(data, beta=beta, mu_prior=prior_mean, Sigma_prior=prior_cov)\n",
    "\n",
    "            time_end = time.time()\n",
    "\n",
    "            Ts_P_total[ith, jth] = time_end - time_start\n",
    "            Ts_P[ith, jth] = time_end - time_mid #just inference/MCMC times\n",
    "    \n",
    "    print(\"Saving...\")\n",
    "    np.savez(f\"./outputs/computation_LRM_theta1={theta1}_theta2={theta2}.npz\", n=Ns, times_total=Ts_P_total, times=Ts_P, data=full_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08586c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
